{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "\n",
    "# ====== ⚙️ 1. 설정 (Settings) ⚙️ =======================================================\n",
    "# --- 데이터 관련 설정 ---\n",
    "days = 100                                  # 처리할 전체 데이터 일수 (일)\n",
    "gap_min = 5                                 # 데이터 간격 (분)\n",
    "delta_min = 30                              # 몇 분 뒤를 예측할 것인지\n",
    "start_idx = 576000                          # 시작 타임 인덱스 (40일 지점)\n",
    "\n",
    "# --- 채널 관련 설정 ---\n",
    "CHANNEL_LIST = [\n",
    "    'tcwv', 't_lowest', 'u_lowest', 'v_lowest',\n",
    "    't850', 'z850', 'u850', 'v850',\n",
    "    't500', 'z500', 'u500', 'v500'\n",
    "]\n",
    "NUM_CHANNELS = len(CHANNEL_LIST)\n",
    "\n",
    "# --- 경로 설정 ---\n",
    "NC_2D_DIR = \"/scratch/x3108a06/nc_data/OUT_2D_nc/\"\n",
    "NC_3D_DIR = \"/scratch/x3108a06/nc_data/OUT_3D_nc/\"\n",
    "\n",
    "var_name = '12ch'\n",
    "base_dir = \"/scratch/x3108a06/input_data/hybrid/\"\n",
    "# 최종 .pt 파일을 저장할 디렉토리\n",
    "output_dir = f'{base_dir}{var_name}_{gap_min}m'\n",
    "stack_dir = f'{base_dir}{var_name}_d{delta_min}m_{gap_min}m'\n",
    "# 통계 파일이 저장된 디렉토리 (위 스크립트와 동일)\n",
    "stats_dir = output_dir\n",
    "\n",
    "# --- 물리 상수 ---\n",
    "G_CONST = 9.80665  # 표준 중력가속도 (m/s^2)\n",
    "# ======================================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ====================================\n",
    "# 평균과 표준편차를 계산하여 파일로 저장합니다.\n",
    "# ====================================\n",
    "\n",
    "\n",
    "def get_12_channels_from_nc_files(timestep_index: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    주어진 타임스텝의 2D와 3D NetCDF 파일에서 12개 채널 데이터를 추출/계산하는 함수.\n",
    "    \"\"\"\n",
    "    file_2d_path = os.path.join(NC_2D_DIR, f\"rcemip_small_awing_96x96x74_1km_6s_300K_48_{timestep_index:010d}.2Dcom_1.nc\")\n",
    "    file_3d_path = os.path.join(NC_3D_DIR, f\"rcemip_small_awing_96x96x74_1km_6s_300K_48_{timestep_index:010d}.nc\")\n",
    "\n",
    "    if not (os.path.exists(file_2d_path) and os.path.exists(file_3d_path)):\n",
    "        return None\n",
    "\n",
    "    with xr.open_dataset(file_2d_path) as ds_2d, xr.open_dataset(file_3d_path) as ds_3d:\n",
    "        tcwv = ds_2d['PW'].isel(time=0).values\n",
    "        z = ds_3d['z'].values\n",
    "        p_hpa = ds_3d['p'].values\n",
    "        pp = ds_3d['PP'].isel(time=0).values\n",
    "        tabs = ds_3d['TABS'].isel(time=0).values\n",
    "        u = ds_3d['U'].isel(time=0).values\n",
    "        v = ds_3d['V'].isel(time=0).values\n",
    "\n",
    "        t_lowest = tabs[0, :, :]\n",
    "        u_lowest = u[0, :, :]\n",
    "        v_lowest = v[0, :, :]\n",
    "\n",
    "        geopotential = G_CONST * z\n",
    "        full_pressure_pa = (p_hpa * 100)[:, np.newaxis, np.newaxis] + pp\n",
    "        \n",
    "        pressure_levels_hpa = [850.0, 500.0]\n",
    "        interpolated_channels = []\n",
    "        ny, nx = tcwv.shape\n",
    "\n",
    "        for p_level_hpa in pressure_levels_hpa:\n",
    "            target_pressure_pa = p_level_hpa * 100.0\n",
    "            t_interp, z_interp, u_interp, v_interp = (np.zeros((ny, nx)) for _ in range(4))\n",
    "\n",
    "            for j in range(ny):\n",
    "                for i in range(nx):\n",
    "                    p_profile = full_pressure_pa[:, j, i]\n",
    "                    p_rev, t_rev, u_rev, v_rev, gp_rev = (\n",
    "                        p_profile[::-1], tabs[:, j, i][::-1], u[:, j, i][::-1],\n",
    "                        v[:, j, i][::-1], geopotential[::-1]\n",
    "                    )\n",
    "                    t_interp[j, i] = np.interp(target_pressure_pa, p_rev, t_rev)\n",
    "                    z_interp[j, i] = np.interp(target_pressure_pa, p_rev, gp_rev)\n",
    "                    u_interp[j, i] = np.interp(target_pressure_pa, p_rev, u_rev)\n",
    "                    v_interp[j, i] = np.interp(target_pressure_pa, p_rev, v_rev)\n",
    "            \n",
    "            interpolated_channels.extend([t_interp, z_interp, u_interp, v_interp])\n",
    "\n",
    "    all_channels = [tcwv, t_lowest, u_lowest, v_lowest] + interpolated_channels\n",
    "    return np.stack(all_channels, axis=0)\n",
    "\n",
    "\n",
    "def calculate_and_save_stats():\n",
    "    \"\"\"메인 실행 함수: 평균 및 표준편차를 계산하고 저장.\"\"\"\n",
    "    print(\"===== 평균 및 표준편차 계산 시작 =====\")\n",
    "    os.makedirs(stats_dir, exist_ok=True)\n",
    "    \n",
    "    mean_array = np.zeros((NUM_CHANNELS,))\n",
    "    mean2_array = np.zeros((NUM_CHANNELS,))\n",
    "    count = 0\n",
    "    \n",
    "    # 훈련 데이터의 70%를 사용하여 통계치 계산\n",
    "    loop_end_idx_train = start_idx + (days * 24 * 60 * 10) * 7 // 10\n",
    "    # 평균 계산 시에는 모든 데이터를 다 볼 필요가 없으므로 간격을 넓게 설정\n",
    "    for i in tqdm(range(start_idx, loop_end_idx_train + 1, gap_min * 100), desc=\"Calculating Mean/Std\"):\n",
    "        data_np = get_12_channels_from_nc_files(i)\n",
    "        if data_np is None: continue\n",
    "\n",
    "        mean_array += data_np.mean(axis=(1, 2))\n",
    "        mean2_array += (data_np**2).mean(axis=(1, 2))\n",
    "        count += 1\n",
    "    \n",
    "    if count == 0:\n",
    "        print(\"오류: 평균/표준편차를 계산할 데이터를 찾지 못했습니다. 파일 경로와 인덱스를 확인하세요.\")\n",
    "        return\n",
    "\n",
    "    mean = mean_array / count\n",
    "    mean2 = mean2_array / count\n",
    "    std = np.sqrt(mean2 - mean**2)\n",
    "\n",
    "    # 계산된 통계치 저장\n",
    "    mean_std_filename = f\"mean_std_{days}d_{gap_min}m.npy\"\n",
    "    mean_std_path = os.path.join(stats_dir, mean_std_filename)\n",
    "    np.save(mean_std_path, np.array([mean, std]))\n",
    "    \n",
    "    print(f\"\\n===== 계산 완료! =====\")\n",
    "    print(f\"평균/표준편차 저장 완료: {mean_std_path}\")\n",
    "    print(\"Mean:\", mean)\n",
    "    print(\"Std:\", std)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    calculate_and_save_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== 데이터 정규화 및 저장 시작 =====\n",
      "통계 파일 불러오기 완료: /scratch/x3108a06/input_data/hybrid/12ch_5m/mean_std_100d_5m.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Normalizing and Saving: 100%|██████████| 28801/28801 [00:02<00:00, 10296.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== 모든 전처리 작업 완료! =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ======================================\n",
    "# 데이터를 정규화, 패딩하고 .pt 파일로 저장합니다.\n",
    "# ======================================\n",
    "\n",
    "\n",
    "def cyclic_pad_2d(x: torch.Tensor, pad: int) -> torch.Tensor:\n",
    "    \"\"\"주기적 경계 조건으로 2D 텐서를 패딩하는 함수.\"\"\"\n",
    "    x = torch.cat([x[..., -pad:, :], x, x[..., :pad, :]], dim=-2)\n",
    "    x = torch.cat([x[..., :, -pad:], x, x[..., :, :pad]], dim=-1)\n",
    "    return x\n",
    "\n",
    "\n",
    "def normalize_and_save_data():\n",
    "    \"\"\"메인 실행 함수: 데이터를 정규화, 패딩하고 저장.\"\"\"\n",
    "    print(\"===== 데이터 정규화 및 저장 시작 =====\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # --- 1. 저장된 평균/표준편차 불러오기 ---\n",
    "    mean_std_filename = f\"mean_std_{days}d_{gap_min}m.npy\"\n",
    "    mean_std_path = os.path.join(stats_dir, mean_std_filename)\n",
    "    \n",
    "    if not os.path.exists(mean_std_path):\n",
    "        print(f\"오류: 통계 파일({mean_std_path})을 찾을 수 없습니다.\")\n",
    "        print(\"먼저 '평균 및 표준편차 계산 스크립트'를 실행해주세요.\")\n",
    "        return\n",
    "        \n",
    "    mean, std = np.load(mean_std_path)\n",
    "    print(f\"통계 파일 불러오기 완료: {mean_std_path}\")\n",
    "    \n",
    "    # Torch 텐서로 변환하여 GPU 사용 준비\n",
    "    mean_tensor = torch.from_numpy(mean).float().view(NUM_CHANNELS, 1, 1)\n",
    "    std_tensor = torch.from_numpy(std).float().view(NUM_CHANNELS, 1, 1)\n",
    "\n",
    "    # --- 2. 전체 데이터를 순회하며 정규화, 패딩 및 저장 ---\n",
    "    loop_end_idx_full = start_idx + days * 24 * 60 * 10\n",
    "    # 모든 데이터를 처리해야 하므로 간격을 촘촘하게 설정\n",
    "    for i in tqdm(range(start_idx, loop_end_idx_full + 1, gap_min * 10), desc=\"Normalizing and Saving\"):\n",
    "        if os.path.exists(os.path.join(output_dir, f\"{i:010d}.pt\")):\n",
    "            continue  # 이미 있으면 다음 파일로 바로 넘어감\n",
    "\n",
    "        data_np = get_12_channels_from_nc_files(i)\n",
    "        if data_np is None: continue\n",
    "        \n",
    "        data_tensor = torch.from_numpy(data_np).float()\n",
    "        \n",
    "        # 정규화 (Standardization)\n",
    "        normalized_tensor = (data_tensor - mean_tensor) / std_tensor\n",
    "        \n",
    "        # 주기적 패딩 적용\n",
    "        padded_tensor = cyclic_pad_2d(normalized_tensor, pad=16)\n",
    "\n",
    "        torch.save(padded_tensor, os.path.join(output_dir, f\"{i:010d}.pt\"))\n",
    "\n",
    "    print(\"\\n===== 모든 전처리 작업 완료! =====\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    normalize_and_save_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20034it [35:51,  9.31it/s]\n"
     ]
    }
   ],
   "source": [
    "# =======================\n",
    "#      (x, y) stack\n",
    "# =======================\n",
    "\n",
    "# x, y를 첫번째 차원으로 쌓아서 (2, C, H, W) 모양의 텐서를 만드는 코드\n",
    "\n",
    "os.makedirs(stack_dir, exist_ok=True)\n",
    "\n",
    "# 파일 리스트 정렬\n",
    "filelist = sorted([f for f in os.listdir(output_dir) if f.endswith('.pt')])\n",
    "N = len(filelist)\n",
    "\n",
    "# 몇번째 뒤와 x, y쌍으로 묶을 것인지\n",
    "shift = delta_min // gap_min\n",
    "\n",
    "for idx, fname in tqdm(enumerate(filelist)):  # type: int, str\n",
    "    # 최종 결과 파일의 경로를 먼저 확인합니다.\n",
    "    output_fpath = os.path.join(stack_dir, fname)\n",
    "\n",
    "    # 만약 결과 파일이 이미 존재한다면, 이번 순서는 건너뜁니다.\n",
    "    if os.path.exists(output_fpath):\n",
    "        continue  # 다음 파일로 바로 넘어감\n",
    "\n",
    "    fpath = os.path.join(output_dir, fname)\n",
    "    data = torch.load(fpath, map_location='cpu')\n",
    "    if not isinstance(data, torch.Tensor):\n",
    "        print(f\"{fname}: Not a tensor, skipped.\")\n",
    "        continue\n",
    "\n",
    "    # 현재 파일\n",
    "    # padded_0 = cyclic_pad_2d(data, 16)\n",
    "\n",
    "    # shift만큼 뒤 파일(없으면 마지막 파일 사용)\n",
    "    shifted_idx = min(idx + shift, N - 1)\n",
    "    shifted_fname = filelist[shifted_idx]\n",
    "    shifted_fpath = os.path.join(output_dir, shifted_fname)\n",
    "    shifted_data = torch.load(shifted_fpath, map_location='cpu')\n",
    "    \n",
    "    # padded_1 = cyclic_pad_2d(shifted_data, 16)\n",
    "\n",
    "    # (2, 128, 128) 저장\n",
    "    out = torch.stack([data, shifted_data], dim=0)\n",
    "    torch.save(out, os.path.join(stack_dir, fname))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12ch_d30m_5m] Total: 20034 -> train: 14023, valid: 3005, test: 3006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 14023/14023 [16:19<00:00, 14.31it/s]\n",
      "valid: 100%|██████████| 3005/3005 [02:05<00:00, 23.90it/s]\n",
      "test: 100%|██████████| 3006/3006 [02:07<00:00, 23.66it/s]\n"
     ]
    }
   ],
   "source": [
    "# =======================\n",
    "#    train test split\n",
    "# =======================\n",
    "\n",
    "import shutil\n",
    "from math import floor\n",
    "\n",
    "\n",
    "# 앞에서부터 70%를 train으로, 15%를 valid로, 15%를 test 셋으로 분리하여 저장하는 코드\n",
    "\n",
    "def split_and_copy(src_dir, dst_base, prefix='', total_limit=None):\n",
    "    os.makedirs(dst_base, exist_ok=True)\n",
    "    files = sorted([f for f in os.listdir(src_dir) if os.path.isfile(os.path.join(src_dir, f))])\n",
    "    if total_limit:\n",
    "        files = files[:total_limit]\n",
    "    n = len(files)\n",
    "    n_train = floor(n * 0.7)\n",
    "    n_valid = floor(n * 0.15)\n",
    "    n_test = n - n_train - n_valid\n",
    "\n",
    "    splits = [\n",
    "        ('train', n_train),\n",
    "        ('valid', n_valid),\n",
    "        ('test', n_test),\n",
    "    ]\n",
    "\n",
    "    print(f\"[{prefix}] Total: {n} -> train: {n_train}, valid: {n_valid}, test: {n_test}\")\n",
    "\n",
    "    idx = 0\n",
    "    for split, count in splits:\n",
    "        dst_dir = os.path.join(dst_base, prefix, split)\n",
    "        os.makedirs(dst_dir, exist_ok=True)\n",
    "        for i in tqdm(range(count), desc=split):\n",
    "            src_path = os.path.join(src_dir, files[idx])\n",
    "            dst_path = os.path.join(dst_dir, files[idx])\n",
    "            idx += 1\n",
    "            if os.path.exists(dst_path):\n",
    "                continue  # 다음 파일로 바로 넘어감\n",
    "            shutil.copy2(src_path, dst_path)\n",
    "\n",
    "\n",
    "# ============== settings ====================\n",
    "split_and_copy(\n",
    "    src_dir=stack_dir,\n",
    "    dst_base=base_dir,\n",
    "    prefix=f'{var_name}_d{delta_min}m_{gap_min}m',\n",
    "    total_limit=None\n",
    ")\n",
    "# ============================================"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
