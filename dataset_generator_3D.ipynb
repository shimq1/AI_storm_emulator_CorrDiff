{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 파일: rcemip_small_awing_96x96x74_1km_6s_300K_48_0000576000.nc ---\n",
      "\n",
      "[파일에 포함된 변수 목록]\n",
      "\n",
      "변수 이름           | 차원 (Dimensions)                | 단위 (Units)\n",
      "----------------------------------------------------------------------\n",
      "x               | ('x',)                         | m\n",
      "y               | ('y',)                         | m\n",
      "z               | ('z',)                         | m\n",
      "time            | ('time',)                      | d\n",
      "p               | ('z',)                         | mb\n",
      "U               | ('time', 'z', 'y', 'x')        | m/s       \n",
      "V               | ('time', 'z', 'y', 'x')        | m/s       \n",
      "W               | ('time', 'z', 'y', 'x')        | m/s       \n",
      "PP              | ('time', 'z', 'y', 'x')        | Pa        \n",
      "QRAD            | ('time', 'z', 'y', 'x')        | K/day     \n",
      "TABS            | ('time', 'z', 'y', 'x')        | K         \n",
      "QV              | ('time', 'z', 'y', 'x')        | g/kg      \n",
      "QN              | ('time', 'z', 'y', 'x')        | g/kg      \n",
      "QP              | ('time', 'z', 'y', 'x')        | g/kg      \n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# 3D nc 파일의 변수 목록 확인하는 코드\n",
    "# ===============================\n",
    "\n",
    "\n",
    "import netCDF4 as nc\n",
    "import os\n",
    "\n",
    "file_path = '/scratch/x3108a06/nc_data/OUT_3D_nc/rcemip_small_awing_96x96x74_1km_6s_300K_48_0000576000.nc'\n",
    "\n",
    "def inspect_netcdf_variables(path):\n",
    "    \"\"\"\n",
    "    지정된 경로의 NetCDF 파일에 포함된 변수들을 출력하는 함수입니다.\n",
    "\n",
    "    Args:\n",
    "        path (str): NetCDF 파일의 경로.\n",
    "    \"\"\"\n",
    "    # 파일이 실제로 존재하는지 확인합니다.\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"오류: 파일을 찾을 수 없습니다. 경로를 확인해주세요: {path}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # 'with' 구문을 사용하면 파일을 사용한 후 자동으로 닫아주어 편리합니다.\n",
    "        # 파일을 읽기 모드('r')로 엽니다.\n",
    "        with nc.Dataset(path, 'r') as dataset:\n",
    "            \n",
    "            # 파일 이름 출력\n",
    "            print(f\"--- 파일: {os.path.basename(path)} ---\")\n",
    "\n",
    "            # 파일에 변수가 있는지 확인합니다.\n",
    "            if not dataset.variables:\n",
    "                print(\"파일에 변수가 없습니다.\")\n",
    "                return\n",
    "            \n",
    "            print(\"\\n[파일에 포함된 변수 목록]\\n\")\n",
    "            \n",
    "            # 보기 좋은 표 형태로 출력하기 위한 헤더\n",
    "            print(f\"{'변수 이름':<15} | {'차원 (Dimensions)':<30} | {'단위 (Units)'}\")\n",
    "            print(\"-\" * 70)\n",
    "\n",
    "            # dataset.variables는 딕셔너리처럼 파일 내의 모든 변수를 담고 있습니다.\n",
    "            # 키(key)는 변수 이름입니다.\n",
    "            for var_name in dataset.variables:\n",
    "                variable = dataset.variables[var_name]\n",
    "                \n",
    "                # 변수의 차원 정보를 튜플 형태로 가져옵니다.\n",
    "                dims = variable.dimensions\n",
    "                \n",
    "                # 변수의 단위 정보를 가져옵니다. 단위 정보가 없으면 'N/A'로 표시합니다.\n",
    "                units = getattr(variable, 'units', 'N/A')\n",
    "                \n",
    "                # 변수 이름, 차원, 단위를 정해진 형식에 맞춰 출력합니다.\n",
    "                print(f\"{var_name:<15} | {str(dims):<30} | {units}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"파일을 읽는 중 오류가 발생했습니다: {e}\")\n",
    "\n",
    "# 함수를 실행하여 결과를 확인합니다.\n",
    "inspect_netcdf_variables(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== 12채널 데이터 전처리 시작 (하이브리드 방식) =====\n",
      "\n",
      "--- 1/2: 평균 및 표준편차 계산 중 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Mean/Std:  16%|█▌        | 325/2017 [03:36<23:00,  1.23it/s]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ====== ⚙️ 1. 설정 (Settings) ⚙️ =======================================================\n",
    "# --- 데이터 관련 설정 ---\n",
    "days = 100                                  # 학습할 데이터 일수 (일)\n",
    "gap_min = 5                                 # 데이터 간격 (분)\n",
    "delta_min = 30                              # 몇 분 뒤를 예측할 것인지\n",
    "start_idx = 576000                          # 시작 타임 인덱스 (40일 지점)\n",
    "\n",
    "# --- 채널 관련 설정 ---\n",
    "# 생성할 12개 채널의 이름 목록\n",
    "CHANNEL_LIST = [\n",
    "    # Single Level (4 channels)\n",
    "    'tcwv', 't_lowest', 'u_lowest', 'v_lowest',\n",
    "    # 850 hPa (4 channels)\n",
    "    't850', 'z850', 'u850', 'v850',\n",
    "    # 500 hPa (4 channels)\n",
    "    't500', 'z500', 'u500', 'v500'\n",
    "]\n",
    "NUM_CHANNELS = len(CHANNEL_LIST)\n",
    "\n",
    "# --- 경로 설정 ---\n",
    "# 입력 데이터가 저장된 기본 경로\n",
    "NC_2D_DIR = \"/scratch/x3108a06/nc_data/OUT_2D_nc/\"\n",
    "NC_3D_DIR = \"/scratch/x3108a06/nc_data/OUT_3D_nc/\"\n",
    "\n",
    "# 전처리된 .pt 파일을 저장할 디렉토리\n",
    "var_name = '12ch'\n",
    "base_dir = \"/scratch/x3108a06/input_data/hybrid/\"\n",
    "input_dir = f'{base_dir}{var_name}_{gap_min}m'\n",
    "pad_dir = f\"{base_dir}{var_name}_d{delta_min}m_{gap_min}m\"\n",
    "\n",
    "# --- 물리 상수 ---\n",
    "G_CONST = 9.80665  # 표준 중력가속도 (m/s^2)\n",
    "# ======================================================================================\n",
    "\n",
    "\n",
    "def get_12_channels_from_nc_files(timestep_index: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    주어진 타임스텝의 2D와 3D NetCDF 파일에서 12개 채널 데이터를 추출/계산하는 함수.\n",
    "    - TCWV는 2D 파일의 PW 변수에서 가져옴.\n",
    "    - 나머지 11개 채널은 3D 파일에서 계산함.\n",
    "\n",
    "    Args:\n",
    "        timestep_index (int): 처리할 파일의 타임스텝 인덱스.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: (12, 96, 96) 모양의 Numpy 배열.\n",
    "    \"\"\"\n",
    "    file_2d_path = os.path.join(NC_2D_DIR, f\"rcemip_small_awing_96x96x74_1km_6s_300K_48_{timestep_index:010d}.2Dcom_1.nc\")\n",
    "    file_3d_path = os.path.join(NC_3D_DIR, f\"rcemip_small_awing_96x96x74_1km_6s_300K_48_{timestep_index:010d}.nc\")\n",
    "\n",
    "    if not (os.path.exists(file_2d_path) and os.path.exists(file_3d_path)):\n",
    "        return None\n",
    "\n",
    "    with xr.open_dataset(file_2d_path) as ds_2d, xr.open_dataset(file_3d_path) as ds_3d:\n",
    "        # --- 1. 단일 레벨 채널 계산 ---\n",
    "        \n",
    "        # 채널 1: 총가강수량 (Total Column Water Vapor)\n",
    "        # 2D 파일의 'PW' 변수를 직접 사용\n",
    "        tcwv = ds_2d['PW'].isel(time=0).values\n",
    "\n",
    "        # 3D 파일에서 나머지 변수 불러오기\n",
    "        z = ds_3d['z'].values\n",
    "        p_hpa = ds_3d['p'].values\n",
    "        pp = ds_3d['PP'].isel(time=0).values\n",
    "        tabs = ds_3d['TABS'].isel(time=0).values\n",
    "        u = ds_3d['U'].isel(time=0).values\n",
    "        v = ds_3d['V'].isel(time=0).values\n",
    "\n",
    "        # 채널 2: 최하층 기온 (Temperature at the lowest level) - 2m 기온 근사\n",
    "        t_lowest = tabs[0, :, :]\n",
    "\n",
    "        # 채널 3: 최하층 동서바람 (Eastward wind at the lowest level) - 10m 바람 근사\n",
    "        u_lowest = u[0, :, :]\n",
    "\n",
    "        # 채널 4: 최하층 남북바람 (Northward wind at the lowest level) - 10m 바람 근사\n",
    "        v_lowest = v[0, :, :]\n",
    "\n",
    "        # --- 2. 등압면 채널 계산 (Pressure Level Channels) ---\n",
    "        geopotential = G_CONST * z\n",
    "        full_pressure_pa = (p_hpa * 100)[:, np.newaxis, np.newaxis] + pp\n",
    "        \n",
    "        pressure_levels_hpa = [850.0, 500.0]\n",
    "        interpolated_channels = []\n",
    "        ny, nx = tcwv.shape\n",
    "\n",
    "        for p_level_hpa in pressure_levels_hpa:\n",
    "            target_pressure_pa = p_level_hpa * 100.0\n",
    "            t_interp, z_interp, u_interp, v_interp = (np.zeros((ny, nx)) for _ in range(4))\n",
    "\n",
    "            for j in range(ny):\n",
    "                for i in range(nx):\n",
    "                    p_profile = full_pressure_pa[:, j, i]\n",
    "                    p_rev, t_rev, u_rev, v_rev, gp_rev = (\n",
    "                        p_profile[::-1], tabs[:, j, i][::-1], u[:, j, i][::-1],\n",
    "                        v[:, j, i][::-1], geopotential[::-1]\n",
    "                    )\n",
    "                    t_interp[j, i] = np.interp(target_pressure_pa, p_rev, t_rev)\n",
    "                    z_interp[j, i] = np.interp(target_pressure_pa, p_rev, gp_rev)\n",
    "                    u_interp[j, i] = np.interp(target_pressure_pa, p_rev, u_rev)\n",
    "                    v_interp[j, i] = np.interp(target_pressure_pa, p_rev, v_rev)\n",
    "            \n",
    "            interpolated_channels.extend([t_interp, z_interp, u_interp, v_interp])\n",
    "\n",
    "    # --- 3. 모든 채널을 하나로 합치기 ---\n",
    "    all_channels = [tcwv, t_lowest, u_lowest, v_lowest] + interpolated_channels\n",
    "    return np.stack(all_channels, axis=0)\n",
    "\n",
    "\n",
    "def cyclic_pad_2d(x: torch.Tensor, pad: int) -> torch.Tensor:\n",
    "    \"\"\"주기적 경계 조건으로 2D 텐서를 패딩하는 함수.\"\"\"\n",
    "    x = torch.cat([x[..., -pad:, :], x, x[..., :pad, :]], dim=-2)\n",
    "    x = torch.cat([x[..., :, -pad:], x, x[..., :, :pad]], dim=-1)\n",
    "    return x\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"메인 실행 함수: 데이터 전처리 및 저장을 수행.\"\"\"\n",
    "    print(\"===== 12채널 데이터 전처리 시작 (하이브리드 방식) =====\")\n",
    "    os.makedirs(input_dir, exist_ok=True)\n",
    "    \n",
    "    # --- 1. 평균(mean) 및 표준편차(std) 계산 ---\n",
    "    print(\"\\n--- 1/2: 평균 및 표준편차 계산 중 ---\")\n",
    "    mean_array = np.zeros((NUM_CHANNELS,))\n",
    "    mean2_array = np.zeros((NUM_CHANNELS,))\n",
    "    count = 0\n",
    "    \n",
    "    loop_end_idx_train = start_idx + (days * 24 * 60 * 10) * 7 // 10\n",
    "    for i in tqdm(range(start_idx, loop_end_idx_train + 1, gap_min * 100), desc=\"Calculating Mean/Std\"):\n",
    "        data_np = get_12_channels_from_nc_files(i)\n",
    "        if data_np is None: continue\n",
    "\n",
    "        mean_array += data_np.mean(axis=(1, 2))\n",
    "        mean2_array += (data_np**2).mean(axis=(1, 2))\n",
    "        count += 1\n",
    "    \n",
    "    if count == 0:\n",
    "        print(\"오류: 평균/표준편차를 계산할 데이터를 찾지 못했습니다. 파일 경로와 인덱스를 확인하세요.\")\n",
    "        return\n",
    "\n",
    "    mean = mean_array / count\n",
    "    mean2 = mean2_array / count\n",
    "    std = np.sqrt(mean2 - mean**2)\n",
    "\n",
    "    mean_std_path = os.path.join(input_dir, f\"mean_std_{days}d_{gap_min}m.npy\")\n",
    "    np.save(mean_std_path, np.array([mean, std]))\n",
    "    print(f\"평균/표준편차 저장 완료: {mean_std_path}\")\n",
    "    print(\"Mean:\", mean)\n",
    "    print(\"Std:\", std)\n",
    "\n",
    "    # --- 2. 정규화, 패딩 및 .pt 파일로 저장 ---\n",
    "    print(\"\\n--- 2/2: 데이터 정규화 및 저장 중 ---\")\n",
    "    loop_end_idx_full = start_idx + days * 24 * 60 * 10\n",
    "    for i in tqdm(range(start_idx, loop_end_idx_full + 1, gap_min * 10), desc=\"Normalizing and Saving\"):\n",
    "        data_np = get_12_channels_from_nc_files(i)\n",
    "        if data_np is None: continue\n",
    "        \n",
    "        data_tensor = torch.from_numpy(data_np).float()\n",
    "        \n",
    "        mean_tensor = torch.from_numpy(mean).float().view(NUM_CHANNELS, 1, 1)\n",
    "        std_tensor = torch.from_numpy(std).float().view(NUM_CHANNELS, 1, 1)\n",
    "        normalized_tensor = (data_tensor - mean_tensor) / std_tensor\n",
    "        \n",
    "        padded_tensor = cyclic_pad_2d(normalized_tensor, pad=16)\n",
    "\n",
    "        torch.save(padded_tensor, os.path.join(input_dir, f\"{i:010d}.pt\"))\n",
    "\n",
    "    print(\"\\n===== 모든 전처리 작업 완료! =====\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "#      (x, y) stack\n",
    "# =======================\n",
    "\n",
    "# x, y를 첫번째 차원으로 쌓아서 (2, C, H, W) 모양의 텐서를 만드는 코드\n",
    "\n",
    "os.makedirs(pad_dir, exist_ok=True)\n",
    "\n",
    "# 파일 리스트 정렬\n",
    "filelist = sorted([f for f in os.listdir(input_dir) if f.endswith('.pt')])\n",
    "N = len(filelist)\n",
    "\n",
    "# 몇번째 뒤와 x, y쌍으로 묶을 것인지\n",
    "shift = delta_min // gap_min\n",
    "\n",
    "for idx, fname in tqdm(enumerate(filelist)):  # type: int, str\n",
    "    # 최종 결과 파일의 경로를 먼저 확인합니다.\n",
    "    output_fpath = os.path.join(pad_dir, fname)\n",
    "\n",
    "    # 만약 결과 파일이 이미 존재한다면, 이번 순서는 건너뜁니다.\n",
    "    if os.path.exists(output_fpath):\n",
    "        continue  # 다음 파일로 바로 넘어감\n",
    "\n",
    "    fpath = os.path.join(input_dir, fname)\n",
    "    data = torch.load(fpath, map_location='cpu')\n",
    "    if not isinstance(data, torch.Tensor):\n",
    "        print(f\"{fname}: Not a tensor, skipped.\")\n",
    "        continue\n",
    "\n",
    "    # 현재 파일\n",
    "    # padded_0 = cyclic_pad_2d(data, 16)\n",
    "\n",
    "    # shift만큼 뒤 파일(없으면 마지막 파일 사용)\n",
    "    shifted_idx = min(idx + shift, N - 1)\n",
    "    shifted_fname = filelist[shifted_idx]\n",
    "    shifted_fpath = os.path.join(input_dir, shifted_fname)\n",
    "    shifted_data = torch.load(shifted_fpath, map_location='cpu')\n",
    "    \n",
    "    # padded_1 = cyclic_pad_2d(shifted_data, 16)\n",
    "\n",
    "    # (2, 128, 128) 저장\n",
    "    out = torch.stack([data, shifted_data], dim=0)\n",
    "    torch.save(out, os.path.join(pad_dir, fname))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PWUV_d30m_1m] Total: 144001 -> train: 100800, valid: 21600, test: 21601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 100800/100800 [00:02<00:00, 46451.95it/s]\n",
      "valid: 100%|██████████| 21600/21600 [00:00<00:00, 46849.77it/s]\n",
      "test: 100%|██████████| 21601/21601 [00:00<00:00, 53621.80it/s]\n"
     ]
    }
   ],
   "source": [
    "# =======================\n",
    "#    train test split\n",
    "# =======================\n",
    "\n",
    "import shutil\n",
    "from math import floor\n",
    "\n",
    "\n",
    "# 앞에서부터 70%를 train으로, 15%를 valid로, 15%를 test 셋으로 분리하여 저장하는 코드\n",
    "\n",
    "def split_and_copy(src_dir, dst_base, prefix='', total_limit=None):\n",
    "    os.makedirs(dst_base, exist_ok=True)\n",
    "    files = sorted([f for f in os.listdir(src_dir) if os.path.isfile(os.path.join(src_dir, f))])\n",
    "    if total_limit:\n",
    "        files = files[:total_limit]\n",
    "    n = len(files)\n",
    "    n_train = floor(n * 0.7)\n",
    "    n_valid = floor(n * 0.15)\n",
    "    n_test = n - n_train - n_valid\n",
    "\n",
    "    splits = [\n",
    "        ('train', n_train),\n",
    "        ('valid', n_valid),\n",
    "        ('test', n_test),\n",
    "    ]\n",
    "\n",
    "    print(f\"[{prefix}] Total: {n} -> train: {n_train}, valid: {n_valid}, test: {n_test}\")\n",
    "\n",
    "    idx = 0\n",
    "    for split, count in splits:\n",
    "        dst_dir = os.path.join(dst_base, prefix, split)\n",
    "        os.makedirs(dst_dir, exist_ok=True)\n",
    "        for i in tqdm(range(count), desc=split):\n",
    "            src_path = os.path.join(src_dir, files[idx])\n",
    "            dst_path = os.path.join(dst_dir, files[idx])\n",
    "            idx += 1\n",
    "            if os.path.exists(dst_path):\n",
    "                continue  # 다음 파일로 바로 넘어감\n",
    "            shutil.copy2(src_path, dst_path)\n",
    "\n",
    "\n",
    "# ============== settings ====================\n",
    "split_and_copy(\n",
    "    src_dir=pad_dir,\n",
    "    dst_base=base_dir,\n",
    "    prefix=f'{var_name}_d{delta_min}m_{gap_min}m',\n",
    "    total_limit=None\n",
    ")\n",
    "# ============================================"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
